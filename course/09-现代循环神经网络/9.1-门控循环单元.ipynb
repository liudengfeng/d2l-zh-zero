{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db9a77eb",
   "metadata": {},
   "source": [
    "# 9.1 门控循环单元（GRU）\n",
    "\n",
    "在 【sec_bptt】中，\n",
    "我们讨论了如何在循环神经网络中计算梯度，\n",
    "以及矩阵连续乘积可以导致梯度消失或梯度爆炸的问题。\n",
    "下面我们简单思考一下这种梯度异常在实践中的意义：\n",
    "\n",
    "* 我们可能会遇到这样的情况：早期观测值对预测所有未来观测值具有非常重要的意义。\n",
    "  考虑一个极端情况，其中第一个观测值包含一个校验和，\n",
    "  目标是在序列的末尾辨别校验和是否正确。\n",
    "  在这种情况下，第一个词元的影响至关重要。\n",
    "  我们希望有某些机制能够在一个记忆元里存储重要的早期信息。\n",
    "  如果没有这样的机制，我们将不得不给这个观测值指定一个非常大的梯度，\n",
    "  因为它会影响所有后续的观测值。\n",
    "* 我们可能会遇到这样的情况：一些词元没有相关的观测值。\n",
    "  例如，在对网页内容进行情感分析时，\n",
    "  可能有一些辅助HTML代码与网页传达的情绪无关。\n",
    "  我们希望有一些机制来*跳过*隐状态表示中的此类词元。\n",
    "* 我们可能会遇到这样的情况：序列的各个部分之间存在逻辑中断。\n",
    "  例如，书的章节之间可能会有过渡存在，\n",
    "  或者证券的熊市和牛市之间可能会有过渡存在。\n",
    "  在这种情况下，最好有一种方法来*重置*我们的内部状态表示。\n",
    "\n",
    "在学术界已经提出了许多方法来解决这类问题。\n",
    "其中最早的方法是\"长短期记忆\"（long-short-term memory，LSTM）\n",
    " :cite:`Hochreiter.Schmidhuber.1997`，\n",
    "我们将在 9.2中讨论。\n",
    "门控循环单元（gated recurrent unit，GRU）\n",
    " :cite:`Cho.Van-Merrienboer.Bahdanau.ea.2014`\n",
    "是一个稍微简化的变体，通常能够提供同等的效果，\n",
    "并且计算 :cite:`Chung.Gulcehre.Cho.ea.2014`的速度明显更快。\n",
    "由于门控循环单元更简单，我们从它开始解读。\n",
    "\n",
    "## 门控隐状态\n",
    "\n",
    "门控循环单元与普通的循环神经网络之间的关键区别在于：\n",
    "前者支持隐状态的门控。\n",
    "这意味着模型有专门的机制来确定应该何时更新隐状态，\n",
    "以及应该何时重置隐状态。\n",
    "这些机制是可学习的，并且能够解决了上面列出的问题。\n",
    "例如，如果第一个词元非常重要，\n",
    "模型将学会在第一次观测之后不更新隐状态。\n",
    "同样，模型也可以学会跳过不相关的临时观测。\n",
    "最后，模型还将学会在需要的时候重置隐状态。\n",
    "下面我们将详细讨论各类门控。\n",
    "\n",
    "### 重置门和更新门\n",
    "\n",
    "我们首先介绍*重置门*（reset gate）和*更新门*（update gate）。\n",
    "我们把它们设计成$(0, 1)$区间中的向量，\n",
    "这样我们就可以进行凸组合。\n",
    "重置门允许我们控制“可能还想记住”的过去状态的数量；\n",
    "更新门将允许我们控制新状态中有多少个是旧状态的副本。\n",
    "\n",
    "我们从构造这些门控开始。 【fig_gru_1】\n",
    "描述了门控循环单元中的重置门和更新门的输入，\n",
    "输入是由当前时间步的输入和前一时间步的隐状态给出。\n",
    "两个门的输出是由使用sigmoid激活函数的两个全连接层给出。\n",
    "\n",
    "![在门控循环单元模型中计算重置门和更新门](../img/gru-1.svg)\n",
    "【fig_gru_1】\n",
    "\n",
    "我们来看一下门控循环单元的数学表达。\n",
    "对于给定的时间步$t$，假设输入是一个小批量\n",
    "$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$\n",
    "（样本个数$n$，输入个数$d$），\n",
    "上一个时间步的隐状态是\n",
    "$\\mathbf{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$\n",
    "（隐藏单元个数$h$）。\n",
    "那么，重置门$\\mathbf{R}_t \\in \\mathbb{R}^{n \\times h}$和\n",
    "更新门$\\mathbf{Z}_t \\in \\mathbb{R}^{n \\times h}$的计算如下所示：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{R}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xr} + \\mathbf{H}_{t-1} \\mathbf{W}_{hr} + \\mathbf{b}_r),\\\\\n",
    "\\mathbf{Z}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xz} + \\mathbf{H}_{t-1} \\mathbf{W}_{hz} + \\mathbf{b}_z),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中$\\mathbf{W}_{xr}, \\mathbf{W}_{xz} \\in \\mathbb{R}^{d \\times h}$\n",
    "和$\\mathbf{W}_{hr}, \\mathbf{W}_{hz} \\in \\mathbb{R}^{h \\times h}$是权重参数，\n",
    "$\\mathbf{b}_r, \\mathbf{b}_z \\in \\mathbb{R}^{1 \\times h}$是偏置参数。\n",
    "请注意，在求和过程中会触发广播机制\n",
    "（请参阅 2.1.3）。\n",
    "我们使用sigmoid函数（如 4.1中介绍的）\n",
    "将输入值转换到区间$(0, 1)$。\n",
    "\n",
    "### 候选隐状态\n",
    "\n",
    "接下来，让我们将重置门$\\mathbf{R}_t$\n",
    "与 :eqref:`rnn_h_with_state`\n",
    "中的常规隐状态更新机制集成，\n",
    "得到在时间步$t$的*候选隐状态*（candidate hidden state）\n",
    "$\\tilde{\\mathbf{H}}_t \\in \\mathbb{R}^{n \\times h}$。\n",
    "\n",
    "$$\\tilde{\\mathbf{H}}_t = \\tanh(\\mathbf{X}_t \\mathbf{W}_{xh} + \\left(\\mathbf{R}_t \\odot \\mathbf{H}_{t-1}\\right) \\mathbf{W}_{hh} + \\mathbf{b}_h),$$\n",
    ":eqlabel:`gru_tilde_H`\n",
    "\n",
    "其中$\\mathbf{W}_{xh} \\in \\mathbb{R}^{d \\times h}$\n",
    "和$\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$是权重参数，\n",
    "$\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$是偏置项，\n",
    "符号$\\odot$是Hadamard积（按元素乘积）运算符。\n",
    "在这里，我们使用tanh非线性激活函数来确保候选隐状态中的值保持在区间$(-1, 1)$中。\n",
    "\n",
    "与 :eqref:`rnn_h_with_state`相比，\n",
    " :eqref:`gru_tilde_H`中的$\\mathbf{R}_t$和$\\mathbf{H}_{t-1}$\n",
    "的元素相乘可以减少以往状态的影响。\n",
    "每当重置门$\\mathbf{R}_t$中的项接近$1$时，\n",
    "我们恢复一个如 :eqref:`rnn_h_with_state`中的普通的循环神经网络。\n",
    "对于重置门$\\mathbf{R}_t$中所有接近$0$的项，\n",
    "候选隐状态是以$\\mathbf{X}_t$作为输入的多层感知机的结果。\n",
    "因此，任何预先存在的隐状态都会被*重置*为默认值。\n",
    "\n",
    " 【fig_gru_2】说明了应用重置门之后的计算流程。\n",
    "\n",
    "![在门控循环单元模型中计算候选隐状态](../img/gru-2.svg)\n",
    "【fig_gru_2】\n",
    "\n",
    "### 隐状态\n",
    "\n",
    "上述的计算结果只是候选隐状态，我们仍然需要结合更新门$\\mathbf{Z}_t$的效果。\n",
    "这一步确定新的隐状态$\\mathbf{H}_t \\in \\mathbb{R}^{n \\times h}$\n",
    "在多大程度上来自旧的状态$\\mathbf{H}_{t-1}$和\n",
    "新的候选状态$\\tilde{\\mathbf{H}}_t$。\n",
    "更新门$\\mathbf{Z}_t$仅需要在\n",
    "$\\mathbf{H}_{t-1}$和$\\tilde{\\mathbf{H}}_t$\n",
    "之间进行按元素的凸组合就可以实现这个目标。\n",
    "这就得出了门控循环单元的最终更新公式：\n",
    "\n",
    "$$\\mathbf{H}_t = \\mathbf{Z}_t \\odot \\mathbf{H}_{t-1}  + (1 - \\mathbf{Z}_t) \\odot \\tilde{\\mathbf{H}}_t.$$\n",
    "\n",
    "每当更新门$\\mathbf{Z}_t$接近$1$时，模型就倾向只保留旧状态。\n",
    "此时，来自$\\mathbf{X}_t$的信息基本上被忽略，\n",
    "从而有效地跳过了依赖链条中的时间步$t$。\n",
    "相反，当$\\mathbf{Z}_t$接近$0$时，\n",
    "新的隐状态$\\mathbf{H}_t$就会接近候选隐状态$\\tilde{\\mathbf{H}}_t$。\n",
    "这些设计可以帮助我们处理循环神经网络中的梯度消失问题，\n",
    "并更好地捕获时间步距离很长的序列的依赖关系。\n",
    "例如，如果整个子序列的所有时间步的更新门都接近于$1$，\n",
    "则无论序列的长度如何，在序列起始时间步的旧隐状态都将很容易保留并传递到序列结束。\n",
    "\n",
    " 【fig_gru_3】说明了更新门起作用后的计算流。\n",
    "\n",
    "![计算门控循环单元模型中的隐状态](../img/gru-3.svg)\n",
    "【fig_gru_3】\n",
    "\n",
    "总之，门控循环单元具有以下两个显著特征：\n",
    "\n",
    "* 重置门有助于捕获序列中的短期依赖关系；\n",
    "* 更新门有助于捕获序列中的长期依赖关系。\n",
    "\n",
    "## 从零开始实现\n",
    "\n",
    "为了更好地理解门控循环单元模型，我们从零开始实现它。\n",
    "首先，我们读取 8.5中使用的时间机器数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8599a833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 获取当前 Notebook 文件的路径\n",
    "notebook_path = os.path.abspath(\".\")\n",
    "\n",
    "# 获取父级目录的路径\n",
    "parent_directory = os.path.dirname(notebook_path)\n",
    "\n",
    "# 将父级目录添加到 sys.path\n",
    "if parent_directory not in sys.path:\n",
    "    sys.path.append(parent_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df4db42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "import d2l\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3ce648",
   "metadata": {},
   "source": [
    "### [**初始化模型参数**]\n",
    "\n",
    "下一步是初始化模型参数。\n",
    "我们从标准差为$0.01$的高斯分布中提取权重，\n",
    "并将偏置项设为$0$，超参数`num_hiddens`定义隐藏单元的数量，\n",
    "实例化与更新门、重置门、候选隐状态和输出层相关的所有权重和偏置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f071cee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def get_params(vocab_size, num_hiddens, device):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape, device=device)*0.01\n",
    "\n",
    "    def three():\n",
    "        return (normal((num_inputs, num_hiddens)),\n",
    "                normal((num_hiddens, num_hiddens)),\n",
    "                d2l.zeros(num_hiddens, device=device))\n",
    "\n",
    "    W_xz, W_hz, b_z = three()  # 更新门参数\n",
    "    W_xr, W_hr, b_r = three()  # 重置门参数\n",
    "    W_xh, W_hh, b_h = three()  # 候选隐状态参数\n",
    "    # 输出层参数\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = d2l.zeros(num_outputs, device=device)\n",
    "    # 附加梯度\n",
    "    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a169aa",
   "metadata": {},
   "source": [
    "### 定义模型\n",
    "\n",
    "现在我们将[**定义隐状态的初始化函数**]`init_gru_state`。\n",
    "与 8.5中定义的`init_rnn_state`函数一样，\n",
    "此函数返回一个形状为（批量大小，隐藏单元个数）的张量，张量的值全部为零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012eff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def init_gru_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d5455c",
   "metadata": {},
   "source": [
    "现在我们准备[**定义门控循环单元模型**]，\n",
    "模型的架构与基本的循环神经网络单元是相同的，\n",
    "只是权重更新公式更为复杂。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e50283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def gru(inputs, state, params):\n",
    "    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)\n",
    "        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)\n",
    "        H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)\n",
    "        H = Z * H + (1 - Z) * H_tilda\n",
    "        Y = H @ W_hq + b_q\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs, dim=0), (H,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c87e0e",
   "metadata": {},
   "source": [
    "### [**训练**]与预测\n",
    "\n",
    "训练和预测的工作方式与 8.5完全相同。\n",
    "训练结束后，我们分别打印输出训练集的困惑度，\n",
    "以及前缀“time traveler”和“traveler”的预测序列上的困惑度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7326532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet, pytorch\n",
    "vocab_size, num_hiddens, device = len(vocab), 256, d2l.try_gpu()\n",
    "num_epochs, lr = 500, 1\n",
    "model = d2l.RNNModelScratch(len(vocab), num_hiddens, device, get_params,\n",
    "                            init_gru_state, gru)\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a4d0f0",
   "metadata": {},
   "source": [
    "## [**简洁实现**]\n",
    "\n",
    "高级API包含了前文介绍的所有配置细节，\n",
    "所以我们可以直接实例化门控循环单元模型。\n",
    "这段代码的运行速度要快得多，\n",
    "因为它使用的是编译好的运算符而不是Python来处理之前阐述的许多细节。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e38c9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "num_inputs = vocab_size\n",
    "gru_layer = nn.GRU(num_inputs, num_hiddens)\n",
    "model = d2l.RNNModel(gru_layer, len(vocab))\n",
    "model = model.to(device)\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4037520e",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 门控循环神经网络可以更好地捕获时间步距离很长的序列上的依赖关系。\n",
    "* 重置门有助于捕获序列中的短期依赖关系。\n",
    "* 更新门有助于捕获序列中的长期依赖关系。\n",
    "* 重置门打开时，门控循环单元包含基本循环神经网络；更新门打开时，门控循环单元可以跳过子序列。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 假设我们只想使用时间步$t'$的输入来预测时间步$t > t'$的输出。对于每个时间步，重置门和更新门的最佳值是什么？\n",
    "1. 调整和分析超参数对运行时间、困惑度和输出顺序的影响。\n",
    "1. 比较`rnn.RNN`和`rnn.GRU`的不同实现对运行时间、困惑度和输出字符串的影响。\n",
    "1. 如果仅仅实现门控循环单元的一部分，例如，只有一个重置门或一个更新门会怎样？\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[Discussions](https://discuss.d2l.ai/t/2763)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
